from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from typing import Dict, Any, List, Optional
from enum import Enum
import streamlit as st
from dotenv import load_dotenv
import os

# .envãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç’°å¢ƒå¤‰æ•°ã‚’èª­ã¿è¾¼ã‚€
load_dotenv()

# APIã‚­ãƒ¼ã‚’å–å¾—
GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')

class BaseModel:
    """ãƒ¢ãƒ‡ãƒ«ã®åŸºåº•ã‚¯ãƒ©ã‚¹"""
    def __init__(self):
        # ç«‹å ´ã®å®šç¾©ï¼ˆå¿…è¦ã«å¿œã˜ã¦å¤‰æ›´å¯èƒ½ï¼‰
        self.position_a = {
            "name": "é©æ–°çš„ãªæ€è€ƒ",
            "emoji": "ğŸš€",
            "focus": "æ–°ã—ã„ã‚¢ã‚¤ãƒ‡ã‚¢ã‚„æ–¬æ–°ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’é‡è¦–",
            "style": "info"
        }
        self.position_b = {
            "name": "ä¿å®ˆçš„ãªæ€è€ƒ",
            "emoji": "ğŸ›¡ï¸",
            "focus": "å®Ÿç¾å¯èƒ½æ€§ã‚„ãƒªã‚¹ã‚¯ã€æ—¢å­˜ã®æ çµ„ã¿ã‚’é‡è¦–",
            "style": "warning"
        }
        self.llm = self._initialize_llm()
        self.chain = self._create_chain()
    
    def _initialize_llm(self) -> ChatGoogleGenerativeAI:
        """LLMã‚’åˆæœŸåŒ–"""
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            google_api_key=GOOGLE_API_KEY,  # APIã‚­ãƒ¼ã‚’æŒ‡å®š
            temperature=0.7,
            convert_system_message_to_human=True
        )
    
    def _create_chain(self) -> LLMChain:
        """ãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ"""
        raise NotImplementedError
    
    def _get_llm_response(self, prompt: str) -> str:
        """LLMã®å¿œç­”ã‚’å–å¾—"""
        response = self.llm.invoke(prompt)
        return response.content if hasattr(response, 'content') else str(response)

class GeminiChainOfThought:
    def __init__(self):
        self.llm = self._initialize_llm()
        self.chain = self._initialize_chain()
    
    def _initialize_llm(self):
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0.7,
            convert_system_message_to_human=True
        )
    
    def _initialize_chain(self):
        return LLMChain(
            llm=self.llm,
            prompt=PromptTemplate(
                input_variables=["question"],
                template="""
                ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€æ®µéšçš„ã«è€ƒãˆã¦å›ç­”ã—ã¦ãã ã•ã„ã€‚

                è³ªå•: {question}

                1. ã¾ãšã€å•é¡Œã‚’ç†è§£ã—ã€å¿…è¦ãªæƒ…å ±ã‚’æ•´ç†ã—ã¾ã™ã€‚
                2. æ¬¡ã«ã€æ®µéšçš„ã«æ¨è«–ã‚’é€²ã‚ã¦ã„ãã¾ã™ã€‚
                3. æœ€å¾Œã«ã€çµè«–ã‚’å°ãå‡ºã—ã¾ã™ã€‚

                å„ã‚¹ãƒ†ãƒƒãƒ—ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
                """
            )
        )
    
    def solve_problem(self, question: str) -> Dict[str, str]:
        """Chain of Thoughtãƒ‘ã‚¿ãƒ¼ãƒ³ã§å•é¡Œã‚’è§£æ±º"""
        # å•é¡Œåˆ†æ
        analysis_result = self.chain.invoke({"question": question})
        
        # æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹æ§‹ç¯‰
        thought_process = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®å•é¡Œåˆ†æã«åŸºã¥ã„ã¦ã€æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã‚’æ§‹ç¯‰ã—ã¦ãã ã•ã„ã€‚\n\n{analysis_result['text']}"
        })
        
        # æ®µéšçš„æ¨è«–
        reasoning_result = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ã«åŸºã¥ã„ã¦ã€æ®µéšçš„ã«æ¨è«–ã—ã¦ãã ã•ã„ã€‚\n\n{thought_process['text']}"
        })
        
        # æœ€çµ‚å›ç­”ç”Ÿæˆ
        final_answer = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®æ¨è«–çµæœã«åŸºã¥ã„ã¦ã€æœ€çµ‚çš„ãªå›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚\n\n{reasoning_result['text']}"
        })
        
        return {
            "analysis": analysis_result['text'],
            "thought_process": thought_process['text'],
            "reasoning": reasoning_result['text'],
            "final_answer": final_answer['text']
        }

class GeminiReasoning:
    def __init__(self):
        self.llm = self._initialize_llm()
        self.chain = self._initialize_chain()
    
    def _initialize_llm(self):
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0.7,
            convert_system_message_to_human=True
        )
    
    def _initialize_chain(self):
        return LLMChain(
            llm=self.llm,
            prompt=PromptTemplate(
                input_variables=["question"],
                template="""
                ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€æ§‹é€ åŒ–ã•ã‚ŒãŸæ¨è«–ã‚’è¡Œã£ã¦ãã ã•ã„ã€‚

                è³ªå•: {question}

                1. å‰ææ¡ä»¶ã‚’æ˜ç¢ºã«ã—ã¾ã™ã€‚
                2. åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚’æ•´ç†ã—ã¾ã™ã€‚
                3. è«–ç†çš„ãªæ¨è«–ã‚’è¡Œã„ã¾ã™ã€‚
                4. çµè«–ã‚’å°ãå‡ºã—ã¾ã™ã€‚

                å„ã‚¹ãƒ†ãƒƒãƒ—ã®çµæœã‚’æ˜ç¢ºã«ç¤ºã—ã¦ãã ã•ã„ã€‚
                """
            )
        )
    
    def direct_reasoning(self, question: str) -> Dict[str, str]:
        """ç›´æ¥æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        # å‰ææ¡ä»¶åˆ†æ
        assumptions = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€å‰ææ¡ä»¶ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚\n\n{question}"
        })
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        data_processing = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®å‰ææ¡ä»¶ã«åŸºã¥ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ã—ã¦ãã ã•ã„ã€‚\n\n{assumptions['text']}"
        })
        
        # æ¨è«–å®Ÿè¡Œ
        reasoning = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ã„ã¦ã€æ¨è«–ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\n\n{data_processing['text']}"
        })
        
        return {
            "assumptions": assumptions['text'],
            "data_processing": data_processing['text'],
            "reasoning": reasoning['text']
        }
    
    def chained_reasoning(self, question: str) -> Dict[str, str]:
        """ãƒã‚§ãƒ¼ãƒ³æ¨è«–ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        # å•é¡Œåˆ†è§£
        decomposition = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®å•é¡Œã‚’åˆ†è§£ã—ã¦ãã ã•ã„ã€‚\n\n{question}"
        })
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†æ
        data_analysis = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®å•é¡Œåˆ†è§£ã«åŸºã¥ã„ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã¦ãã ã•ã„ã€‚\n\n{decomposition['text']}"
        })
        
        # ä»®å®šè¨­å®š
        assumptions = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿åˆ†æã«åŸºã¥ã„ã¦ã€ä»®å®šã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚\n\n{data_analysis['text']}"
        })
        
        # æ¨è«–å®Ÿè¡Œ
        final_result = self.chain.invoke({
            "question": f"ä»¥ä¸‹ã®ä»®å®šã«åŸºã¥ã„ã¦ã€æœ€çµ‚çš„ãªçµè«–ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚\n\n{assumptions['text']}"
        })
        
        return {
            "decomposition": decomposition['text'],
            "data_analysis": data_analysis['text'],
            "assumptions": assumptions['text'],
            "final_result": final_result['text']
        }

class EvaluatorOptimizer:
    def __init__(self):
        self.generator = self._initialize_llm("generator")
        self.evaluator = self._initialize_llm("evaluator")
        self.optimizer = self._initialize_llm("optimizer")
    
    def _initialize_llm(self, role: str):
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash-lite",
            temperature=0.7,
            convert_system_message_to_human=True
        )
    
    def generate_optimized_response(self, question: str) -> Dict[str, Any]:
        """Evaluator-Optimizerãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼"""
        iterations = []
        
        # åˆæœŸå›ç­”ç”Ÿæˆ
        initial_response = self.generator.invoke({
            "question": question
        })
        iterations.append({
            "iteration": 1,
            "response": initial_response['text']
        })
        
        # è©•ä¾¡
        evaluation = self.evaluator.invoke({
            "question": f"ä»¥ä¸‹ã®å›ç­”ã‚’è©•ä¾¡ã—ã€æ”¹å–„ç‚¹ã‚’æŒ‡æ‘˜ã—ã¦ãã ã•ã„ã€‚\n\n{initial_response['text']}"
        })
        iterations[0]["evaluation"] = evaluation['text']
        
        # æœ€é©åŒ–
        optimized_response = self.optimizer.invoke({
            "question": f"ä»¥ä¸‹ã®è©•ä¾¡ã«åŸºã¥ã„ã¦ã€å›ç­”ã‚’æœ€é©åŒ–ã—ã¦ãã ã•ã„ã€‚\n\n{evaluation['text']}"
        })
        iterations[0]["optimized_response"] = optimized_response['text']
        
        return {
            "iterations": iterations,
            "final_response": optimized_response['text']
        }

class DebateBasedCooperation(BaseModel):
    def _create_chain(self) -> LLMChain:
        """ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒã‚§ãƒ¼ãƒ³ã‚’ä½œæˆ"""
        return LLMChain(
            llm=self.llm,
            prompt=PromptTemplate(
                input_variables=["question", "position_a_name", "position_b_name"],
                template="""
                ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦ã€{position_a_name}ã¨{position_b_name}ã®ä¸¡æ–¹ã®è¦–ç‚¹ã‹ã‚‰è­°è«–ã‚’è¡Œã„ã€æœ€é©ãªå›ç­”ã‚’å°ãå‡ºã—ã¦ãã ã•ã„ã€‚
                è³ªå•: {question}
                """
            )
        )

    def generate_debate_response(self, question: str) -> Dict[str, Any]:
        """ãƒ‡ã‚£ãƒ™ãƒ¼ãƒˆãƒ™ãƒ¼ã‚¹ã®å”èª¿ãƒ‘ã‚¿ãƒ¼ãƒ³"""
        try:
            # ç«‹å ´Aã‹ã‚‰ã®æ„è¦‹
            position_a_opinion = str(self._get_llm_response(
                f"{self.position_a['name']}ã®è¦–ç‚¹ã‹ã‚‰ä»¥ä¸‹ã®è³ªå•ã«ã¤ã„ã¦æ„è¦‹ã‚’è¿°ã¹ã¦ãã ã•ã„ã€‚{self.position_a['focus']}ã—ã¦ãã ã•ã„ã€‚\nè³ªå•: {question}"
            ))
            
            # ç«‹å ´Bã‹ã‚‰ã®åè«–
            position_b_rebuttal = str(self._get_llm_response(
                f"{self.position_b['name']}ã®è¦–ç‚¹ã‹ã‚‰ä»¥ä¸‹ã®æ„è¦‹ã«å¯¾ã—ã¦åè«–ã—ã¦ãã ã•ã„ã€‚{self.position_b['focus']}ã—ã¦ãã ã•ã„ã€‚\næ„è¦‹: {position_a_opinion}"
            ))
            
            # ç«‹å ´Aã‹ã‚‰ã®å†åè«–
            position_a_rebuttal = str(self._get_llm_response(
                f"{self.position_a['name']}ã®è¦–ç‚¹ã‹ã‚‰ä»¥ä¸‹ã®åè«–ã«å¯¾ã—ã¦å†åè«–ã—ã¦ãã ã•ã„ã€‚{self.position_a['focus']}ã—ã¦ãã ã•ã„ã€‚\nåè«–: {position_b_rebuttal}"
            ))
            
            # ç«‹å ´Bã‹ã‚‰ã®å†åè«–
            position_b_final_rebuttal = str(self._get_llm_response(
                f"{self.position_b['name']}ã®è¦–ç‚¹ã‹ã‚‰ä»¥ä¸‹ã®å†åè«–ã«å¯¾ã—ã¦æœ€çµ‚çš„ãªåè«–ã‚’ã—ã¦ãã ã•ã„ã€‚{self.position_b['focus']}ã—ã¦ãã ã•ã„ã€‚\nå†åè«–: {position_a_rebuttal}"
            ))
            
            # åˆæ„å½¢æˆ
            consensus = str(self._get_llm_response(
                f"ä»¥ä¸‹ã®è­°è«–ã‚’è¸ã¾ãˆã¦ã€{self.position_a['name']}ã¨{self.position_b['name']}ã®ä¸¡æ–¹ã‚’è€ƒæ…®ã—ãŸåˆæ„å½¢æˆã‚’è¡Œã£ã¦ãã ã•ã„ã€‚\n\n{self.position_a['name']}ã®æ„è¦‹: {position_a_opinion}\n{self.position_b['name']}ã®åè«–: {position_b_rebuttal}\n{self.position_a['name']}ã®å†åè«–: {position_a_rebuttal}\n{self.position_b['name']}ã®æœ€çµ‚åè«–: {position_b_final_rebuttal}"
            ))
            
            return {
                "iterations": [{
                    "iteration": 1,
                    "position_a_opinion": position_a_opinion,
                    "position_b_rebuttal": position_b_rebuttal,
                    "position_a_rebuttal": position_a_rebuttal,
                    "position_b_final_rebuttal": position_b_final_rebuttal,
                    "position_a": self.position_a,
                    "position_b": self.position_b
                }],
                "final_response": consensus
            }
        except Exception as e:
            st.error(f"Error in Debate-based Cooperation: {str(e)}")
            return {"final_response": "å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"} 